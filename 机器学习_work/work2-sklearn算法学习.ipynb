{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 通用包导入\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## sklearn自带数据集的导入练习\n",
    "sklearn.datasets中包括大量的流行数据集供学习使用<br>\n",
    "主要通过load_xx和fetch_xx两种方式来导入使用<br>\n",
    "分别对应小数据集和大数据集<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 鸢尾花数据集，查看特征，目标，样本量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# 导入方式\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 创建对象接收数据\n",
    "li = load_iris()\n",
    "\n",
    "# 通过DESCR获取整个数据集内容的描述\n",
    "print(li.DESCR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据特征值:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "数据特征值类型:\n",
      " <class 'numpy.ndarray'>\n",
      "数据特征值规模:\n",
      " (150, 4)\n",
      "目标值:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "特征值名称:\n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "目标值名称:\n",
      " ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 对象拥有的属性有数据特征值,data;目标值,target;\n",
    "# 特征名,feature_names;目标名称.target_names\n",
    "print('数据特征值:\\n', li.data)\n",
    "print('数据特征值类型:\\n', type(li.data))\n",
    "print('数据特征值规模:\\n', li.data.shape)\n",
    "print('目标值:\\n', li.target)\n",
    "print('特征值名称:\\n', li.feature_names)\n",
    "print('目标值名称:\\n', li.target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取数据集后,进一步的就需要对数据集进行切分<br>\n",
    "将数据集具体分为训练集的特征值,目标值;测试集的特征值,目标值<br>\n",
    "切分方法--train_test_split,记x为特征值,y为目标值<br>\n",
    "返回值的顺序一定是x_train,x_test,y_train,y_test<br>\n",
    "传入参数为数据集特征值,数据集目标值,test_size数据集大小切分多少百分比给测试集,random_state随机数种子"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的特征值:\n",
      " [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]]\n",
      "训练集的目标值:\n",
      " [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "训练集特征值规模:\n",
      " (112, 4)\n",
      "--------------------------------------------------\n",
      "测试集的特征值:\n",
      " [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]]\n",
      "测试集的目标值:\n",
      " [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "测试集特征值规模:\n",
      " (38, 4)\n"
     ]
    }
   ],
   "source": [
    "# 导入切分数据包\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 输出切分结果\n",
    "print('训练集的特征值:\\n', x_train)\n",
    "print('训练集的目标值:\\n', y_train)\n",
    "print('训练集特征值规模:\\n', x_train.shape)\n",
    "print('-' * 50)\n",
    "print('测试集的特征值:\\n', x_test)\n",
    "print('测试集的目标值:\\n', y_test)\n",
    "print('测试集特征值规模:\\n', x_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K近邻算法处理鸾尾花数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# 先对数据进行标准化,统一度量\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "# 对测试集和训练集的特征值进行标准化\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "K近邻算法,通过设置k值来调整结果好坏<br>\n",
    "算法训练模拟的过程总结下来就是fit,predict,score三部曲<br>\n",
    "fit传入训练集的特征值与目标值<br>\n",
    "predict传入测试集的特征值,返回预测结果<br>\n",
    "socre传入测试集的特征值和目标值,给出准确率"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集的预期结果是:\n",
      " [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "--------------------------------------------------\n",
      "预测的准确率: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 创建类\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train, y_train)\n",
    "y_predict = knn.predict(x_test)\n",
    "print('测试集的预期结果是:\\n', y_predict)\n",
    "print('-' * 50)\n",
    "print('预测的准确率:', knn.score(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "网格搜素与交叉验证法来获取最佳预测模型<br>\n",
    "1. 创建可能目标参数集合供网格搜索\n",
    "1. 遍历所有训练样本数可能数\n",
    "1. 要注意训练样本数与交叉验证组相关,若交叉验证cv为n,则可供遍历的k值最大为S*(n-1/n),S为原训练样本总数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上准确率： 1.0\n",
      "在交叉验证当中最好的结果： 0.9557312252964426\n",
      "选择最好的模型是： KNeighborsClassifier(n_neighbors=6)\n"
     ]
    }
   ],
   "source": [
    "param = {'n_neighbors': [i for i in range(1, 50)]}\n",
    "# 导入包\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 创建类\n",
    "# 传入参数,具体算法对象,算法参数,cv控制交叉验证的分组数,cv>=2\n",
    "gsc = GridSearchCV(knn, param_grid=param, cv=5)\n",
    "\n",
    "# 处理步骤仍是fit,predict,score\n",
    "gsc.fit(x_train, y_train)\n",
    "\n",
    "print(\"在测试集上准确率：\", gsc.score(x_test, y_test))\n",
    "\n",
    "print(\"在交叉验证当中最好的结果：\", gsc.best_score_)\n",
    "\n",
    "print(\"选择最好的模型是：\", gsc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K近邻算法练习2--Facebook预测签到\n",
    "数据及题目要求:[facebook预测签到](https://www.kaggle.com/c/facebook-v-predicting-check-ins)<br>\n",
    "简介:给出用户当前的坐标值,预测用户最有可能前往的建筑"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       x       y  accuracy    time    place_id\n",
      "0       0  0.7941  9.0809        54  470702  8523065625\n",
      "1       1  5.9567  4.7968        13  186555  1757726713\n",
      "2       2  8.3078  7.0407        74  322648  1137537235\n",
      "3       3  7.3665  2.5165        65  704587  6567393236\n",
      "4       4  4.0961  1.1307        31  472130  7440663949\n",
      "5       5  3.8099  1.9586        75  178065  6289802927\n",
      "6       6  6.3336  4.3720        13  666829  9931249544\n",
      "7       7  5.7409  6.7697        85  369002  5662813655\n",
      "8       8  4.3114  6.9410         3  166384  8471780938\n",
      "9       9  6.3414  0.0758        65  400060  1253803156\n",
      "(29118021, 6)\n"
     ]
    }
   ],
   "source": [
    "# 利用pandas读取数据\n",
    "data = pd.read_csv('./FBlocation/train.csv')\n",
    "# 查看数据内容与规模\n",
    "print(data.head(10))\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600        1970-01-01 18:09:40\n",
      "957        1970-01-10 02:11:10\n",
      "4345       1970-01-05 15:08:02\n",
      "4735       1970-01-06 23:03:03\n",
      "5580       1970-01-09 11:26:50\n",
      "                   ...        \n",
      "29100203   1970-01-01 10:33:56\n",
      "29108443   1970-01-07 23:22:04\n",
      "29109993   1970-01-08 15:03:14\n",
      "29111539   1970-01-04 00:53:41\n",
      "29112154   1970-01-08 23:01:07\n",
      "Name: time, Length: 17710, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# 处理数据,由于数据量太大,电脑可能带不动,因此减小数据集范围\n",
    "# 原数据集x,y的范围为[0,10]\n",
    "data = data.query(\"x > 1.0 &  x < 1.25 & y > 2.5 & y < 2.75\")\n",
    "\n",
    "# 处理时间的数据\n",
    "time_value = pd.to_datetime(data['time'], unit='s')\n",
    "\n",
    "print(time_value)  #最大时间是1月10号\n",
    "# 数据量从29118021--变为17710"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',\n",
      "               '1970-01-05 15:08:02', '1970-01-06 23:03:03',\n",
      "               '1970-01-09 11:26:50', '1970-01-02 16:25:07',\n",
      "               '1970-01-04 15:52:57', '1970-01-01 10:13:36',\n",
      "               '1970-01-09 15:26:06', '1970-01-08 23:52:02',\n",
      "               ...\n",
      "               '1970-01-07 10:03:36', '1970-01-09 11:44:34',\n",
      "               '1970-01-04 08:07:44', '1970-01-04 15:47:47',\n",
      "               '1970-01-08 01:24:11', '1970-01-01 10:33:56',\n",
      "               '1970-01-07 23:22:04', '1970-01-08 15:03:14',\n",
      "               '1970-01-04 00:53:41', '1970-01-08 23:01:07'],\n",
      "              dtype='datetime64[ns]', name='time', length=17710, freq=None)\n",
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[17710 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# 进一步将日期转为DatatimeIndex类型,方便取其中的年月日时分\n",
    "time_value = pd.DatetimeIndex(time_value)\n",
    "print(time_value)\n",
    "# 向数据中添加日,时,分等特征\n",
    "# 观察数据可发现,年月都一样,因此并无分析价值,可以猜测,周末,饭点,晚上等时间特征可能会影响选择\n",
    "data['day'] = time_value.day\n",
    "data['hour'] = time_value.hour\n",
    "data['weekday'] = time_value.weekday\n",
    "\n",
    "# 再删除掉提取信息完,无用的时间特征\n",
    "del data['time']\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805, 7)\n",
      "(239, 8)\n"
     ]
    }
   ],
   "source": [
    "# 进一步处理数据\n",
    "# 把签到数量少于n个目标位置删除\n",
    "place_count = data.groupby('place_id').count()\n",
    "print(place_count.shape)\n",
    "# 把index变为0,1,2，3,4,5,6这种效果，从零开始拍，原来的index是row_id\n",
    "#只选择去的人大于3的数据，认为1,2,3的是噪音\n",
    "tf = place_count[place_count.row_id > 3].reset_index()\n",
    "print(tf.shape)\n",
    "# 处理后,得到推荐场所数从805--239"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            row_id       x       y  accuracy    place_id  day  hour  weekday\n",
      "600            600  1.2214  2.7023        17  6683426742    1    18        3\n",
      "957            957  1.1832  2.6891        58  6683426742   10     2        5\n",
      "4345          4345  1.1935  2.6550        11  6889790653    5    15        0\n",
      "4735          4735  1.1452  2.6074        49  6822359752    6    23        1\n",
      "5580          5580  1.0089  2.7287        19  1527921905    9    11        4\n",
      "...            ...     ...     ...       ...         ...  ...   ...      ...\n",
      "29100203  29100203  1.0129  2.6775        12  3312463746    1    10        3\n",
      "29108443  29108443  1.1474  2.6840        36  3533177779    7    23        2\n",
      "29109993  29109993  1.0240  2.7238        62  6424972551    8    15        3\n",
      "29111539  29111539  1.2032  2.6796        87  3533177779    4     0        6\n",
      "29112154  29112154  1.1070  2.5419       178  4932578245    8    23        3\n",
      "\n",
      "[16918 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# 再将处理后的推荐场所应用回数据\n",
    "data = data[data['place_id'].isin(tf.place_id)]\n",
    "print(data)\n",
    "# 数据量进一步由17710--16918"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16918, 5)\n",
      "Index(['x', 'y', 'day', 'hour', 'weekday'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 删除无用row_id\n",
    "del data['row_id']\n",
    "del data['accuracy']\n",
    "# 分割数据,将目标值与特征值分离\n",
    "# 推荐场所id为目标值\n",
    "y = data['place_id']\n",
    "# 删除目标值,剩下即为特征值\n",
    "x = data.drop(['place_id'], axis=1)\n",
    "# 查看特征值规模\n",
    "print(x.shape)\n",
    "# 查看特征名称\n",
    "print(x.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# 对数据进行分离,训练集与测试集,并对多种特征进行标准化\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "x_train = std.fit_transform(x_train)\n",
    "x_test = std.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上的准确率: 0.491725768321513\n",
      "交叉验证中的最好结果: 0.46666144921152136\n",
      "选择最好的模型是: KNeighborsClassifier(n_neighbors=7)\n"
     ]
    }
   ],
   "source": [
    "# 创建k近邻算法对象,同时采用交叉验证与网格搜索\n",
    "knn=KNeighborsClassifier()\n",
    "param = {'n_neighbors':[i for i in range(3,20)]}\n",
    "gsc=GridSearchCV(knn,param_grid=param,cv=3)\n",
    "\n",
    "gsc.fit(x_train,y_train)\n",
    "\n",
    "# 输出结果\n",
    "print('在测试集上的准确率:',gsc.score(x_test,y_test))\n",
    "print('交叉验证中的最好结果:',gsc.best_score_)\n",
    "print('选择最好的模型是:',gsc.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "项目总结:<br>\n",
    "1. 本预测结果准确率仅适用于小范围数据测试,再加大x,y的范围后,准确率便会降低很多\n",
    "因此K近邻算法从整体意义上来说并不是十分适配本测验\n",
    "2. 仅对于本题所选的范围内,特征值的选取上,一开始加入了minute,准确率为0.42左右\n",
    "去掉minute后,准确率达到了0.484左右.可以推测出,分钟特征对于该模型无意义\n",
    "3. 进一步删除accuracy特征后,准确率进一步上升至0.491,因此该特征对模型也不是十分重要"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-近邻算法总结\n",
    "1. k值的取值影响<br>\n",
    "> 1. k 值取很小：容易受异常点影响; k值取很大：容易受最近数据太多导致比例变化<br>\n",
    "> 1. 在实际应用中，K 值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是 把训练数据在分成两组:训练集和验证集）来选择最优的 K 值。\n",
    "1. k近邻算法的优点\n",
    "> 1. 算法简单，理论成熟，既可以用来做分类也可以用来做回归\n",
    "> 2. 可用于非线性分类。(Y=kx 是线性）\n",
    "> 3. 没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练时间复杂度为 0。\n",
    "> 4. 由于 KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合。\n",
    "> 5. 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。\n",
    "3. k近邻算法的缺点\n",
    "> 1. 需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。\n",
    "> 2. 需要大量的内存，空间复杂度高\n",
    "> 3. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。\n",
    "> 4. 是 lazy learning 方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。(因为每次预测都需要去计算)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 分类模型的评估\n",
    "1. 通过预测的准确率进行评估\n",
    "> estimator.score() , 即预测结果正确的百分比<br>\n",
    "> estimator可以理解为各种预测算法对象,专业称作估计器\n",
    "2. 混沌矩阵\n",
    "> 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)\n",
    "\n",
    "|  真实\\预测   | 正例 |反例 |\n",
    "| -- | -- |--- |\n",
    "| 正例  | 真正例TP   |伪反例FN |\n",
    "| 反例   | 伪正例FP  |真反例TN |\n",
    "\n",
    "通过混沌矩阵可以得到的指标有\n",
    "3. 精确率(Precision)与召回率(Rcall)，FI-socre\n",
    "> 精确率：预测结果为正例样本中真实为正例的比例（查得准）<br>\n",
    "> =TP/(TP+FP)<br>\n",
    "> 召回率：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）\n",
    "> =TP/(TP+FN)<br>\n",
    "> FI-score则是两个指标的结合考量\n",
    "> =2TP/(2TP+FP+FN)\n",
    "4. sklearn内置评估API计算,sklearn.metrics.classification_report\n",
    "> sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )<br>\n",
    "> y_true：真实目标值 <br>\n",
    "> y_pred：估计器预测目标值 <br>\n",
    "> labels:指定类别对应的数字 <br>\n",
    "> target_names：目标类别名称<br>\n",
    "> return：每个类别精确率与召回率\n",
    "5. 通过 ROC 和 AUC,衡量样本不均衡下的评估\n",
    "> ROC曲线是指以FPR为横坐标，TPR为纵坐标所绘的曲线\n",
    "> TPR：所有正类中，有多少被预测成正类。TPR=TP/(TP+FN)\n",
    "> FPR：所有反类中，有多少被预测成正类。FPR=FP/(FP+FN),越小越好\n",
    "> AUC即ROC曲线下方的面积，越大越好！"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 分类算法--朴素贝叶斯算法\n",
    "朴素贝叶斯公式\n",
    "> P(C|W)=P(W|C)*P(C)/P(W)<br>\n",
    "> w为给定文档的特征值(频数统计,预测文档提供)，c 为文档类别<br>\n",
    "> P(C)：每个文档类别的概率(某文档类别词数／总文档词数)<br>\n",
    "> P(W|C)：给定类别下特征（被预测文档中出现的词）的概率<br>\n",
    "> 拉普拉斯平滑处理一些特征出现概率为0的情况<br>\n",
    "> 加入系数α,一般为 1，m为训练文档中统计出的类别/标签个数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 案例实战--文章分类预测\n",
    "数据采用的sklearn自带的数据集fetch_20newsgroups<br>\n",
    "对于此类大数据要采用fetch获取"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "[10  3 17 ...  3  1  7]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# subset=all指训练集与测试集均加载,默认情况下仅有train训练集\n",
    "news = fetch_20newsgroups(subset='all', data_home='data')\n",
    "\n",
    "print(len(news.data))\n",
    "print(news.target)\n",
    "print(news.target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'ñaustin' 'ýé' 'ÿhooked']\n",
      "153196\n"
     ]
    }
   ],
   "source": [
    "# 进行数据分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 对数据集进行特征抽取\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "x_train = tf.fit_transform(x_train)\n",
    "\n",
    "print(tf.get_feature_names_out())\n",
    "print(len(tf.get_feature_names_out()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153196\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "预测的文章类别为： [16 19 18 ... 13  7 14]\n",
      "准确率为： 0.8518675721561969\n",
      "每个类别的精确率和召回率：                           precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.91      0.77      0.83       199\n",
      "           comp.graphics       0.83      0.79      0.81       242\n",
      " comp.os.ms-windows.misc       0.89      0.83      0.86       263\n",
      "comp.sys.ibm.pc.hardware       0.80      0.83      0.81       262\n",
      "   comp.sys.mac.hardware       0.90      0.88      0.89       234\n",
      "          comp.windows.x       0.92      0.85      0.88       230\n",
      "            misc.forsale       0.96      0.67      0.79       257\n",
      "               rec.autos       0.90      0.87      0.88       265\n",
      "         rec.motorcycles       0.90      0.95      0.92       251\n",
      "      rec.sport.baseball       0.89      0.96      0.93       226\n",
      "        rec.sport.hockey       0.95      0.98      0.96       262\n",
      "               sci.crypt       0.76      0.97      0.85       257\n",
      "         sci.electronics       0.84      0.80      0.82       229\n",
      "                 sci.med       0.97      0.86      0.91       249\n",
      "               sci.space       0.92      0.96      0.94       256\n",
      "  soc.religion.christian       0.55      0.98      0.70       243\n",
      "      talk.politics.guns       0.76      0.96      0.85       234\n",
      "   talk.politics.mideast       0.93      0.99      0.96       224\n",
      "      talk.politics.misc       0.98      0.56      0.72       197\n",
      "      talk.religion.misc       0.97      0.26      0.41       132\n",
      "\n",
      "                accuracy                           0.85      4712\n",
      "               macro avg       0.88      0.84      0.84      4712\n",
      "            weighted avg       0.87      0.85      0.85      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试集特征提取\n",
    "x_test = tf.transform(x_test)\n",
    "print(len(tf.get_feature_names_out()))\n",
    "\n",
    "# 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "\n",
    "print(x_train.toarray())\n",
    "# 训练\n",
    "mlt.fit(x_train, y_train)\n",
    "\n",
    "y_predict = mlt.predict(x_test)\n",
    "\n",
    "print(\"预测的文章类别为：\", y_predict)\n",
    "\n",
    "# 得出准确率,这个是很难提高准确率，为什么呢？\n",
    "print(\"准确率为：\", mlt.score(x_test, y_test))\n",
    "# 目前这个场景我们不需要召回率，support是划分为那个类别的有多少个样本\n",
    "print(\"每个类别的精确率和召回率：\", classification_report(y_test, y_predict, target_names=news.target_names))\n",
    "# support得到的是真实的y_test中对应类有多少个"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0类的AUC指标： 0.8827602448315142\n",
      "第1类的AUC指标： 0.8924436555919167\n",
      "第2类的AUC指标： 0.9115266642565895\n",
      "第3类的AUC指标： 0.9079423621236813\n",
      "第4类的AUC指标： 0.9353544202807267\n",
      "第5类的AUC指标： 0.924078924393225\n",
      "第6类的AUC指标： 0.8337324826300182\n",
      "第7类的AUC指标： 0.931151380409095\n",
      "第8类的AUC指标： 0.9711894408467899\n",
      "第9类的AUC指标： 0.979402980363688\n",
      "第10类的AUC指标： 0.9870889441633072\n",
      "第11类的AUC指标： 0.9754571220200273\n",
      "第12类的AUC指标： 0.8957712152751734\n",
      "第13类的AUC指标： 0.9310547140387677\n",
      "第14类的AUC指标： 0.9761592515709157\n",
      "第15类的AUC指标： 0.9659492415515388\n",
      "第16类的AUC指标： 0.9709281463412772\n",
      "第17类的AUC指标： 0.9914096320346321\n",
      "第18类的AUC指标： 0.7815044043824589\n",
      "第19类的AUC指标： 0.6286787084822019\n"
     ]
    }
   ],
   "source": [
    "# ROC曲线与AUC指标计算\n",
    "# 把0-19总计20个分类，变为0和1\n",
    "for i in range(20):\n",
    "    test = np.where(y_test == i, 1, 0)\n",
    "    predict = np.where(y_predict == i, 1, 0)\n",
    "    # roc_auc_score的y_test只能是二分类,针对多分类如何计算AUC\n",
    "    print(f\"第{i}类的AUC指标：\", roc_auc_score(test, predict))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 朴素贝叶斯算法总结\n",
    "朴素贝叶斯分类优缺点\n",
    "> - 优点：<br>\n",
    "> 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。<br>\n",
    "> 对缺失数据不太敏感，算法也比较简单，常用于文本分类。<br>\n",
    "> 分类准确度高，速度快<br>\n",
    "> - 缺点： <br>\n",
    "> 需要知道先验概率 P(F1,F2,…|C)，因此在某些时候会由于假设的先验 模型的原因导致预测效果不佳。<br>\n",
    "> 假设了文章当中一些词语另外一些是独立没关系—-如果有关系，会造成不太靠谱 <br>\n",
    "> 训练集当中去进行统计词这些工作 文章收集的不好，比如有作弊文章，充斥某个词会对结果造成干扰<br>\n",
    "> - 朴素贝叶斯:文本分类—主要应用领域<br>\n",
    "> 神经网络效果要更好（深度学习）\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}