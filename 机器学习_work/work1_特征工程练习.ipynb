{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 特征化练习1--提取字典类数据\n",
    "# 使用DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dictvec():\n",
    "    # 字典类的数据\n",
    "    dict_data=[{'fruit':'apple','price':5,'discount':['1','2','3']},\n",
    "               {'fruit':'orange','price':3,'discount':['2','3']},\n",
    "               {'fruit':'pear','price':6,'discount':['3']}]\n",
    "    # 创建一个DictVectorizer类,创建时要指出是否进行压缩存储\n",
    "    # 默认sparse为True即不进行压缩存储\n",
    "    dict=DictVectorizer(sparse=False)\n",
    "\n",
    "    # 通过fit_transform来讲字典类数据进行转换\n",
    "    data=dict.fit_transform(dict_data)\n",
    "    # 打印查看转换效果\n",
    "    print(data)\n",
    "    print('-'*50)\n",
    "    # 通过get_feature_name_out获取转换后数据的特征类型有哪些\n",
    "    print(dict.get_feature_names_out())\n",
    "    print('-'*50)\n",
    "    # 通过inverse_transform(data),将特征逆转回原数据,知晓特征所指的具体含义\n",
    "    print(dict.inverse_transform(data))\n",
    "\n",
    "dictvec()"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 0. 5.]\n",
      " [0. 1. 1. 0. 1. 0. 3.]\n",
      " [0. 0. 1. 0. 0. 1. 6.]]\n",
      "--------------------------------------------------\n",
      "['discount=1' 'discount=2' 'discount=3' 'fruit=apple' 'fruit=orange'\n",
      " 'fruit=pear' 'price']\n",
      "--------------------------------------------------\n",
      "[{'discount=1': 1.0, 'discount=2': 1.0, 'discount=3': 1.0, 'fruit=apple': 1.0, 'price': 5.0}, {'discount=2': 1.0, 'discount=3': 1.0, 'fruit=orange': 1.0, 'price': 3.0}, {'discount=3': 1.0, 'fruit=pear': 1.0, 'price': 6.0}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "转换规则总结:<br>\n",
    "字典类的键值对会形成一个特征,若键值均为字符串,那么二者合二为一成为一个特征记录,以one-hot编码记录<br>\n",
    "若键为字符串,值为数值型,则键作为一种特征,对应特征数据就是值<br>\n",
    "若值为列表型,列表内只能为字符串类型,并且键会与列表内的值一一对应组成一个特征类型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n",
      "--------------------------------------------------\n",
      "[[0 2 1 0 2 0 1]\n",
      " [1 1 1 0 1 1 1]\n",
      " [0 1 1 2 0 0 0]]\n",
      "--------------------------------------------------\n",
      "['forever' 'hello' 'love' 'programe' 'python' 'the' 'world']\n",
      "--------------------------------------------------\n",
      "[array(['hello', 'world', 'python', 'love'], dtype='<U8'), array(['hello', 'world', 'python', 'love', 'the', 'forever'], dtype='<U8'), array(['hello', 'love', 'programe'], dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "# 英文文本特征抽取\n",
    "# 使用CountVectorizer进行简单的计数再提取\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def countvec():\n",
    "    # 实例化CountVectorizer\n",
    "    # 默认单个字母的单词不计入,默认认为这个词对整个样本无影响\n",
    "    # 在实例化时也可以传入参数max_df或min_df,规定单词的出现次数来对计数后的单词进行删选\n",
    "    count=CountVectorizer()\n",
    "    # 英文文本数据\n",
    "    text=[\"hello world, hello python, i love python\",\n",
    "          \"hello python, i love the world forever\",\n",
    "          \"hello programe, i love programe\"]\n",
    "    # 通过fit_transform来对数据进行转换\n",
    "    data=count.fit_transform(text)\n",
    "\n",
    "    # 输出查看处理效果\n",
    "    # 默认输出的形式是按压缩矩阵的形式输出\n",
    "    print(data)\n",
    "    print('-'*50)\n",
    "    # 通过toarray更方便的查看数据\n",
    "    print(data.toarray())\n",
    "    print('-'*50)\n",
    "    # 通过get_feature_name_out获取转换后数据的特征类型有哪些\n",
    "    print(count.get_feature_names_out())\n",
    "    print('-'*50)\n",
    "    # 通过inverse_transform(data),将特征逆转回原数据,知晓特征所指的具体含义\n",
    "    print(count.inverse_transform(data))\n",
    "\n",
    "countvec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "规则总结:<br>\n",
    "CountVectorizer是通过空格标点等将英文文本分割为一个个独立的单词,并对单词进行计数<br>\n",
    "统计除单个字母单词之外的所有单词作为特征类型,输出每一个样本中所含对应特征类型的次数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python' '不用' '人生漫长' '人生苦短' '喜欢']\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "[[2 0 0 1 1]\n",
      " [1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 中文文本处理\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 汉字文本特征化,CountVectorizer进行空格分割汉字不太合理\n",
    "def countvec():\n",
    "    \"\"\"\n",
    "    对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    data = cv.fit_transform([\"人生苦短，我 喜欢 python python\", \"人生漫长，不用 python\"])\n",
    "\n",
    "    print(cv.get_feature_names_out())\n",
    "\n",
    "    print(data)\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "countvec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于中文文本而言,CountVectorizer自带的分割逻辑不太适用<br>\n",
    "这时候可以导入jieba的中文分词包来辅助处理,先利用包进行分词后再计数转换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 30 34\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "  (0, 6)\t2\n",
      "  (0, 26)\t2\n",
      "  (0, 22)\t2\n",
      "  (0, 12)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 20)\t1\n",
      "  (1, 18)\t3\n",
      "  (1, 28)\t2\n",
      "  (1, 23)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 35)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 34)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 4)\t4\n",
      "  (2, 25)\t1\n",
      "  (2, 5)\t3\n",
      "  (2, 1)\t1\n",
      "  (2, 29)\t2\n",
      "  (2, 13)\t1\n",
      "  (2, 30)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 33)\t1\n",
      "[[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "def cut_word():\n",
    "    # 通过jieba.cut来对中文文本进行更加合理的分词\n",
    "    # cut分词后得到是一个生成器类\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "\n",
    "    con3 = jieba.cut(\"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "\n",
    "    # 先将生成器转为列表来查看分词效果,且利于下部操作\n",
    "    con1=list(con1)\n",
    "    con2=list(con2)\n",
    "    con3=list(con3)\n",
    "    print(len(con1),len(con2),len(con3))\n",
    "    print(con1)\n",
    "    print(con2)\n",
    "    print(con3)\n",
    "    print('-'*50)\n",
    "    # 转为list后的另一个好处就在于,可以通过str.join(list)\n",
    "    # 将切分好的中文文本列表通姑空格 连接起来,这样就可以完美符合CountVectorizer按空格分词的特性进行特征化\n",
    "    text1=' '.join(con1)\n",
    "    text2=' '.join(con2)\n",
    "    text3=' '.join(con3)\n",
    "    return text1,text2,text3\n",
    "\n",
    "def chinese_text_vec():\n",
    "    # 对结巴分词后的中文文本进行转换\n",
    "    # 接受数据\n",
    "    t1,t2,t3=cut_word()\n",
    "    # 创建类\n",
    "    cv = CountVectorizer()\n",
    "    # 转换数据，提取特征\n",
    "    data = cv.fit_transform([t1,t2,t3])\n",
    "    # 输出结果\n",
    "    print(cv.get_feature_names_out())\n",
    "    print(data)\n",
    "    print(data.toarray())\n",
    "\n",
    "chinese_text_vec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF判定法\n",
    "通过计算中文文本转化后特征的tf-idf值，来判定这个特征的重要性<br>\n",
    "tf值是值该特征词在文本中的出现频率<br>\n",
    "idf值叫做逆文档频率，即分析一个词在多少个文本有出现，若出现次数越多，idf值越小<br>\n",
    "计算规则为idf=lg（D/j),D为文本总数，j为包含该词的文本数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 30 34\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "  (0, 20)\t0.2182178902359924\n",
      "  (0, 2)\t0.2182178902359924\n",
      "  (0, 27)\t0.2182178902359924\n",
      "  (0, 19)\t0.2182178902359924\n",
      "  (0, 24)\t0.2182178902359924\n",
      "  (0, 14)\t0.2182178902359924\n",
      "  (0, 31)\t0.2182178902359924\n",
      "  (0, 32)\t0.2182178902359924\n",
      "  (0, 12)\t0.2182178902359924\n",
      "  (0, 22)\t0.4364357804719848\n",
      "  (0, 26)\t0.4364357804719848\n",
      "  (0, 6)\t0.4364357804719848\n",
      "  (1, 34)\t0.24108220270067757\n",
      "  (1, 17)\t0.24108220270067757\n",
      "  (1, 35)\t0.24108220270067757\n",
      "  (1, 9)\t0.24108220270067757\n",
      "  (1, 3)\t0.24108220270067757\n",
      "  (1, 8)\t0.24108220270067757\n",
      "  (1, 7)\t0.24108220270067757\n",
      "  (1, 23)\t0.24108220270067757\n",
      "  (1, 28)\t0.48216440540135513\n",
      "  (1, 18)\t0.5500476874707075\n",
      "  (2, 33)\t0.15698297076974738\n",
      "  (2, 15)\t0.15698297076974738\n",
      "  (2, 10)\t0.15698297076974738\n",
      "  (2, 30)\t0.15698297076974738\n",
      "  (2, 13)\t0.15698297076974738\n",
      "  (2, 29)\t0.31396594153949475\n",
      "  (2, 1)\t0.15698297076974738\n",
      "  (2, 5)\t0.47094891230924213\n",
      "  (2, 25)\t0.15698297076974738\n",
      "  (2, 4)\t0.6279318830789895\n",
      "  (2, 21)\t0.15698297076974738\n",
      "  (2, 0)\t0.15698297076974738\n",
      "  (2, 11)\t0.15698297076974738\n",
      "  (2, 16)\t0.15698297076974738\n",
      "  (2, 18)\t0.11938959557761185\n",
      "--------------------------------------------------\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 计算上述中文文本特征化后的tf-idf值\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfivec():\n",
    "    # 接收切割好的文本数据\n",
    "    t1,t2,t3=cut_word()\n",
    "    # 创建类\n",
    "    # 创建过程中可传入一定的参数，如smooth_idf,默认为True，即对计算后的数据进行一定的平滑性处理\n",
    "    # 旨在针对与出现次数为0的词进行处理，计算规则处理为idf=lg（D+1/j+1) + 1\n",
    "    tfi=TfidfVectorizer()\n",
    "    # 转换数据，计算tfidf值\n",
    "    data=tfi.fit_transform([t1,t2,t3])\n",
    "    # 输出结果\n",
    "    print(tfi.get_feature_names_out())\n",
    "    print('-'*50)\n",
    "    print(data)\n",
    "    print('-'*50)\n",
    "    print(data.toarray())\n",
    "\n",
    "tfivec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 特征处理\n",
    "### 归一化处理--MinMaxScaler\n",
    "### 标准化处理--StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def mm():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    :return: NOne\n",
    "    \"\"\"\n",
    "    # 归一化容易受极值的影响,默认feature_range为(0,1)\n",
    "    mm = MinMaxScaler(feature_range=(-1, 1))\n",
    "    mm1= MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "    data1 = mm1.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "\n",
    "    print('-1--1范围内归一化处理结果\\n',data)\n",
    "    print('-'*50)\n",
    "    print('0--1范围内归一化处理结果\\n',data1)\n",
    "\n",
    "    return None\n",
    "\n",
    "mm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "归一化处理总结:<br>\n",
    "归一化易受极值影响,但计算简便快速<br>\n",
    "归一化在数据量较小,不存在极端数据情况时效果较好<br>\n",
    "归一化处理值计算：x'=(x-min)/max-min--(0,1)之间的范围<br>\n",
    "范围转换扩展计算：x''=x'*(mx-mi)+mi,得到(mi,mx)的新的区间范围<br>\n",
    "当然这些都可以通过定义时,feature_range来直接设置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化后所得数据\n",
      " [[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n",
      "--------------------------------------------------\n",
      "均值\n",
      " [2.33333333 3.         1.33333333]\n",
      "--------------------------------------------------\n",
      "方差\n",
      " [1.55555556 8.66666667 2.88888889]\n",
      "--------------------------------------------------\n",
      "样本数\n",
      " 3\n",
      "--------------------------------------------------\n",
      "均值\n",
      " [0. 0. 0.]\n",
      "--------------------------------------------------\n",
      "方差\n",
      " [1.         1.         1.00000001]\n",
      "--------------------------------------------------\n",
      "样本数\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def stand():\n",
    "    \"\"\"\n",
    "    标准化缩放，不是标准正太分布，只均值为0，方差为1的分布\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 创建标准化类\n",
    "    std = StandardScaler()\n",
    "    # 进行数据转换\n",
    "    data = std.fit_transform([[1., -1., 3.], [2., 4., 2.], [4., 6., -1.]])\n",
    "\n",
    "    print('标准化后所得数据\\n',data)\n",
    "    print('-'*50)\n",
    "\n",
    "    # 均值\n",
    "    print('均值\\n',std.mean_)\n",
    "    print('-'*50)\n",
    "    # 方差\n",
    "    print('方差\\n',std.var_)\n",
    "    print('-'*50)\n",
    "    # 样本数\n",
    "    print('样本数\\n',std.n_samples_seen_)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 对处理后的数据再次标准化转换，反过来验证\n",
    "    data1 = std.fit_transform([[-1.06904497, -1.35873244, 0.98058068],\n",
    "                               [-0.26726124, 0.33968311, 0.39223227],\n",
    "                               [1.33630621, 1.01904933, -1.37281295]])\n",
    "    # 查看处理后的特征的期望与方差是否符合预期\n",
    "    # 均值\n",
    "    print('均值\\n',std.mean_)\n",
    "    print('-'*50)\n",
    "    # 方差\n",
    "    print('方差\\n',std.var_)\n",
    "    print('-'*50)\n",
    "    # 样本数\n",
    "    print('样本数\\n',std.n_samples_seen_)\n",
    "    return None\n",
    "\n",
    "stand()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "标准化缩放总结:<br>\n",
    "计算公式: x'=(x-mean)/σ,mean为均值,σ为标准差<br>\n",
    "标准化是指,把原始数据通过处理后得到期望为0,方差为1的标准化分布数据集,并不是标准正态分布<br>\n",
    "相对于归一化,并不受极值的影响,更适合大规模的数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据处理中缺失值的处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.        ]\n",
      " [3.66666667 3.        ]\n",
      " [7.         6.        ]\n",
      " [3.         2.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy\n",
    "#下面是填补，针对删除，可以用pd和np\n",
    "def im():\n",
    "    \"\"\"\n",
    "    缺失值处理\n",
    "    :return:None\n",
    "    \"\"\"\n",
    "    # NaN, nan,缺失值必须是这种形式，如果是？号(或者其他符号)，就要replace换成这种\n",
    "    # 创建SimpleImputer类\n",
    "    # 其中参数,missing_values指定要替换的数据,strategy指定替换后的数据\n",
    "    # 可以传入字符串表示的计算方法,如['mean', 'median', 'most_frequent', 'constant']\n",
    "    im = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    data = im.fit_transform([[1, 2], [np.nan, 3], [7, 6], [3, 2]])\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "im()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 降维处理\n",
    "降维直接说就是减少特征数<br>\n",
    "从而可以提高模型训练速度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [4]\n",
      " [1]]\n",
      "The surport is [2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def var():\n",
    "    \"\"\"\n",
    "    特征选择-删除低方差的特征\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    #默认只删除方差为0,threshold是方差阈值，删除比这个值小的那些特征\n",
    "    # 删除方差较小的特征值,因为大量方差较小的特征,说明这一特征的特点都相差不大,没有训练区分的必要\n",
    "    var = VarianceThreshold(threshold=1)\n",
    "\n",
    "    data = var.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "\n",
    "    print(data)\n",
    "    # 获得剩余的特征的列编号\n",
    "    print('The surport is %s' % var.get_support(True))\n",
    "    return None\n",
    "\n",
    "var()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}